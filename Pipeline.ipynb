{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtBlf2FnnbDu"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# üîÑ Montar Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ17-uyupWKd"
      },
      "source": [
        "#triplet loss entrenamiento y guardado de embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-S0Aa0qpZzG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import kagglehub\n",
        "\n",
        "# === Ruta destino en tu Drive\n",
        "dataset_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/Dataset\"\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# === Descargar el dataset desde Kaggle\n",
        "print(\"Descargando dataset...\")\n",
        "download_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
        "print(f\"Descarga completa en: {download_path}\")\n",
        "\n",
        "# === Copiar el dataset a tu carpeta en Drive\n",
        "for filename in os.listdir(download_path):\n",
        "    src_file = os.path.join(download_path, filename)\n",
        "    dst_file = os.path.join(dataset_dir, filename)\n",
        "    if os.path.isdir(src_file):\n",
        "        shutil.copytree(src_file, dst_file, dirs_exist_ok=True)\n",
        "    else:\n",
        "        shutil.copy2(src_file, dst_file)\n",
        "\n",
        "print(f\"Dataset copiado a: {dataset_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsPtX1Ompe7u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Sampler\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8gWo2VnpjGw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFkZ63fLplvg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ruta = \"/content/drive/MyDrive/Mendoza_Serey(2025)/Datasets\" # Nueva ruta\n",
        "data = []\n",
        "\n",
        "# Leer todas las im√°genes de todas las clases\n",
        "for category in os.listdir(ruta):\n",
        "    category_path = os.path.join(ruta, category)\n",
        "    if os.path.isdir(category_path):\n",
        "        images = glob.glob(os.path.join(category_path, \"*.*\"))\n",
        "        data.extend([{\"path\": img, \"category\": category} for img in images])\n",
        "\n",
        "# Convertir a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Dividir en train y test (80%/20%) estratificado por categor√≠a\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df[\"category\"], random_state=42)\n",
        "\n",
        "# Mostrar resumen\n",
        "print(\"Clases en train:\\n\", df_train['category'].value_counts())\n",
        "print(\"Clases en test:\\n\",  df_test['category'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V5wmOFdplxL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# Aseg√∫rate de que df_train ya fue generado desde la estructura actual\n",
        "clases = df_train['category'].unique()\n",
        "num_ejemplos = 6\n",
        "\n",
        "fig, axes = plt.subplots(len(clases), num_ejemplos + 1, figsize=((num_ejemplos + 1) * 2, len(clases) * 2))\n",
        "fig.suptitle(\"TRAINING DATA\", fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, clase in enumerate(clases):\n",
        "    muestras = df_train[df_train['category'] == clase].sample(num_ejemplos, random_state=42)\n",
        "    axes[i, 0].text(0.5, 0.5, clase, fontsize=14, fontweight='bold', ha='center', va='center')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    for j, ruta in enumerate(muestras['path']):\n",
        "        try:\n",
        "            img = Image.open(ruta).convert(\"RGB\")\n",
        "            axes[i, j + 1].imshow(img)\n",
        "            axes[i, j + 1].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error abriendo {ruta}: {e}\")\n",
        "            axes[i, j + 1].axis('off')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ZbNxkipl1F"
      },
      "outputs": [],
      "source": [
        "class BrainDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(self.df['category'].unique())\n",
        "        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['path']).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.class_to_idx[row['category']]\n",
        "        return img, label\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.1,0.1,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "dataset_full = BrainDataset(df_train, transform=train_transform)\n",
        "test_ds      = BrainDataset(df_test,  transform=val_transform)\n",
        "\n",
        "val_frac = 0.10\n",
        "n_total  = len(dataset_full)\n",
        "n_val    = int(n_total * val_frac)\n",
        "n_train  = n_total - n_val\n",
        "\n",
        "train_ds, val_ds = random_split(dataset_full, [n_train, n_val],\n",
        "                                 generator=torch.Generator().manual_seed(42))\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YudrHCBipzbU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9wTVO-XHEsF"
      },
      "outputs": [],
      "source": [
        "class BalancedBatchSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Devuelve lotes con 'k' ejemplos por clase.\n",
        "    - labels: lista/array con la etiqueta de cada √≠ndice del dataset\n",
        "    - batch_size: m√∫ltiplo del n¬∫ de clases\n",
        "    - oversample: si True, repite √≠ndices de clases minoritarias para que no se agoten\n",
        "    - shuffle: baraja los √≠ndices al inicio de cada √©poca\n",
        "    - drop_last: si True, descarta el √∫ltimo lote si no est√° completo\n",
        "    \"\"\"\n",
        "    def __init__(self, labels, batch_size, oversample=True, shuffle=True, drop_last=True):\n",
        "        self.labels   = np.array(labels)\n",
        "        self.classes  = np.unique(self.labels)\n",
        "        self.C        = len(self.classes)\n",
        "        assert batch_size % self.C == 0, f\"batch_size debe ser m√∫ltiplo de {self.C}\"\n",
        "        self.k        = batch_size // self.C\n",
        "        self.oversamp = oversample\n",
        "        self.shuffle  = shuffle\n",
        "        self.drop_last= drop_last\n",
        "        self.idcs_by_class = {c: np.where(self.labels==c)[0].tolist() for c in self.classes}\n",
        "        if self.oversamp:\n",
        "            self._oversample()\n",
        "\n",
        "    def _oversample(self):\n",
        "        max_len = max(len(lst) for lst in self.idcs_by_class.values())\n",
        "        for c, lst in self.idcs_by_class.items():\n",
        "            deficit = max_len - len(lst)\n",
        "            if deficit>0:\n",
        "                lst.extend(random.choices(lst, k=deficit))\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            for lst in self.idcs_by_class.values():\n",
        "                random.shuffle(lst)\n",
        "        ptr = {c:0 for c in self.classes}\n",
        "        finished = False\n",
        "        while not finished:\n",
        "            batch = []\n",
        "            for c in self.classes:\n",
        "                start, end = ptr[c], ptr[c] + self.k\n",
        "                if end>len(self.idcs_by_class[c]):\n",
        "                    finished = True\n",
        "                    break\n",
        "                batch.extend(self.idcs_by_class[c][start:end])\n",
        "                ptr[c] = end\n",
        "            if len(batch)==self.k*self.C:\n",
        "                yield batch\n",
        "            elif not self.drop_last and batch:\n",
        "                yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(v)//self.k for v in self.idcs_by_class.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIZBYtEDHOmM"
      },
      "outputs": [],
      "source": [
        "orig_ds      = train_ds.dataset  # BrainDataset original\n",
        "train_idx    = train_ds.indices  # √≠ndices del Subset de entrenamiento\n",
        "train_labels = [\n",
        "    orig_ds.class_to_idx[orig_ds.df.iloc[i]['category']]\n",
        "    for i in train_idx\n",
        "]\n",
        "\n",
        "batch_size   = 44\n",
        "num_classes  = len(orig_ds.classes)\n",
        "k            = batch_size // num_classes  # ejemplos por clase\n",
        "\n",
        "# Instanciar el sampler balanceado personalizado\n",
        "sampler = BalancedBatchSampler(\n",
        "    labels = train_labels,\n",
        "    batch_size = batch_size,\n",
        "    oversample = True,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "# Crear DataLoaders usando batch_sampler\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_sampler = sampler,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "print(\"DataLoaders preparados:\")\n",
        "print(\" ‚Ä¢ Train samples:\", len(train_ds))\n",
        "print(\" ‚Ä¢ Train batches:\", len(train_loader))\n",
        "print(\" ‚Ä¢ Val batches:  \", len(val_loader))\n",
        "print(\" ‚Ä¢ Test batches: \", len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olYWkZjTHOup"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Distribuci√≥n por batch (primeros 5):\")\n",
        "for i, (_, labels) in enumerate(train_loader):\n",
        "    counts = Counter(labels.tolist())\n",
        "    print(f\"Batch {i}: {counts}\")\n",
        "    if i >= 4:\n",
        "        break\n",
        "\n",
        "# Contar total de muestras por clase en toda la √©poca\n",
        "total_counts = Counter()\n",
        "for _, labels in train_loader:\n",
        "    total_counts.update(labels.tolist())\n",
        "print(\"Distribuci√≥n total por √©poca:\", total_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7IxTK6kHZcC"
      },
      "outputs": [],
      "source": [
        "class BackboneEmbedder(nn.Module):\n",
        "    def __init__(self, model_name=\"inception_v4\"):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True)\n",
        "        classifier = self.backbone.get_classifier()\n",
        "        self.in_features = classifier.in_features\n",
        "        self.backbone.reset_classifier(0)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.in_features, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, return_all=False):\n",
        "        x_backbone = self.backbone(x)\n",
        "        x1 = self.relu(self.fc1(x_backbone))\n",
        "        x2 = self.relu(self.fc2(x1))\n",
        "        x3 = self.relu(self.fc3(x2))\n",
        "        x4 = self.fc4(x3)\n",
        "\n",
        "        if return_all:\n",
        "            return {\n",
        "                'backbone': x_backbone,\n",
        "                '512': x1,\n",
        "                '256': x2,\n",
        "                '128': x3,\n",
        "                '64': x4\n",
        "            }\n",
        "        else:\n",
        "            return x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BqH9hc8pzqd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BackboneEmbedder(model_name=\"inception_v4\").to(device)\n",
        "\n",
        "# Convertir lista de tensores a tensor de lote\n",
        "images_tensor = torch.stack(images).to(device)\n",
        "\n",
        "# Ejecutar modelo\n",
        "outputs = model(images_tensor, return_all=True)\n",
        "\n",
        "# Mostrar resultados\n",
        "for key, tensor in outputs.items():\n",
        "    print(f\"{key}: shape = {tensor.shape}\")\n",
        "    print(tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3DGqpsnpztW"
      },
      "outputs": [],
      "source": [
        "pip install pytorch_metric_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek8JCfVXqgNj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C75ND1oYXU5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Ruta donde se guard√≥ el modelo entrenado\n",
        "save_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/Modelos/mri\"\n",
        "model_path = os.path.join(save_dir, 'best_model_triplet_cifar.pth')\n",
        "\n",
        "# Crear la instancia del modelo\n",
        "model = BackboneEmbedder(model_name=\"efficientnet_b0\").to(device)\n",
        "\n",
        "# Cargar pesos entrenados\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Poner en modo evaluaci√≥n (opcional, pero recomendado si no vas a seguir entrenando)\n",
        "model.eval()\n",
        "\n",
        "print(\"Modelo cargado correctamente desde:\", model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX6t48v4qg7o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9qejH7CHc-J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from pytorch_metric_learning.losses import TripletMarginLoss\n",
        "from pytorch_metric_learning.miners import TripletMarginMiner\n",
        "from pytorch_metric_learning.samplers import MPerClassSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modelo base\n",
        "model = BackboneEmbedder(model_name=\"efficientnet_b0\").to(device)\n",
        "\n",
        "# --- Definir miner y p√©rdida ---\n",
        "margin = 0.2\n",
        "miner = TripletMarginMiner(margin=margin, type_of_triplets=\"hard\")\n",
        "criterion = TripletMarginLoss(margin=margin)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1)\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/Modelos/mri\"\n",
        "num_epochs = 50\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# --- Sampler para entrenamiento balanceado ---\n",
        "train_labels = [label for _, label in train_ds]\n",
        "sampler = MPerClassSampler(train_labels, m=4, length_before_new_iter=len(train_ds))\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, sampler=sampler)\n",
        "\n",
        "# --- Entrenamiento ---\n",
        "for epoch in tqdm(range(1, num_epochs + 1), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        embeddings = model(images)\n",
        "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "        hard_triplets = miner(embeddings, labels)\n",
        "\n",
        "        # Saltar batch si no hay triplets hard\n",
        "        if len(hard_triplets[0]) == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(embeddings, labels, hard_triplets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        if epoch == 1:\n",
        "            print(f\"[Depuraci√≥n] Triplets encontrados: {len(hard_triplets[0])}\")\n",
        "            print(f\"[Depuraci√≥n] Loss en batch: {loss.item():.4f}\")\n",
        "            print(f\"[Depuraci√≥n] Embedding std: {embeddings.std().item():.4f}\")\n",
        "\n",
        "    train_loss = running_loss / len(train_ds)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # --- Validaci√≥n ---\n",
        "    model.eval()\n",
        "    val_running = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            embeddings = model(images)\n",
        "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "            hard_triplets = miner(embeddings, labels)\n",
        "            if len(hard_triplets[0]) == 0:\n",
        "                continue\n",
        "\n",
        "            v_loss = criterion(embeddings, labels, hard_triplets).item()\n",
        "            val_running += v_loss * images.size(0)\n",
        "\n",
        "    val_loss = val_running / len(val_ds)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    tqdm.write(\n",
        "        f\"Epoch {epoch}/{num_epochs}  \"\n",
        "        f\"Train Loss: {train_loss:.4f}  \"\n",
        "        f\"Val Loss: {val_loss:.4f}  \"\n",
        "        f\"LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
        "    )\n",
        "\n",
        "    # Early stopping + checkpoint\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, 'best_model_triplet.pth'))\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            tqdm.write(f\"Deteniendo tras {patience} √©pocas sin mejora.\")\n",
        "            break\n",
        "\n",
        "# --- Graficar p√©rdidas ---\n",
        "epochs = list(range(1, len(train_losses) + 1))\n",
        "plt.figure()\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.plot(epochs, val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjSa_Qtlqu_b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TnIN6n8JAUU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Ruta al modelo guardado\n",
        "save_model_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/Modelos/mri\"\n",
        "model_path = os.path.join(save_model_dir, 'best_model_triplet_cifar.pth')\n",
        "\n",
        "# Crear el modelo y mover a GPU si est√° disponible\n",
        "model = BackboneEmbedder(model_name=\"efficientnet_b0\").to(device)\n",
        "\n",
        "# Cargar pesos del modelo\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Poner en modo evaluaci√≥n\n",
        "model.eval()\n",
        "\n",
        "print(f\"Modelo cargado correctamente desde: {model_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9xIGRnmqwG1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egYnlbyGZG3r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# === Ruta donde guardar los archivos\n",
        "save_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "def extract_embeddings_dict(loader, model):\n",
        "    backbone_list = []\n",
        "    dim512_list = []\n",
        "    dim256_list = []\n",
        "    dim128_list = []\n",
        "    dim64_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader):\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs, return_all=True)\n",
        "\n",
        "            backbone_list.extend(outputs['backbone'].cpu().numpy())\n",
        "            dim512_list.extend(outputs['512'].cpu().numpy())\n",
        "            dim256_list.extend(outputs['256'].cpu().numpy())\n",
        "            dim128_list.extend(outputs['128'].cpu().numpy())\n",
        "            dim64_list.extend(outputs['64'].cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'backbone': backbone_list,\n",
        "        '512': dim512_list,\n",
        "        '256': dim256_list,\n",
        "        '128': dim128_list,\n",
        "        '64': dim64_list,\n",
        "        'label': labels_list\n",
        "    })\n",
        "\n",
        "# === Ejecutar y guardar ===\n",
        "df_train = extract_embeddings_dict(train_loader, model)\n",
        "df_val   = extract_embeddings_dict(val_loader, model)\n",
        "df_test  = extract_embeddings_dict(test_loader, model)\n",
        "\n",
        "df_train.to_pickle(os.path.join(save_dir, \"embeddings_vectors_train_mri.pkl\"))\n",
        "df_val.to_pickle(os.path.join(save_dir, \"embeddings_vectors_val_mri.pkl\"))\n",
        "df_test.to_pickle(os.path.join(save_dir, \"embeddings_vectors_test_mri.pkl\"))\n",
        "\n",
        "print(\"Embeddings guardados correctamente en:\", save_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiBgofPqz1g"
      },
      "source": [
        "# evaluacion de otros clasificadores clasificadores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-smpsVcq2FO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaPvi0YUFRmc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# === Rutas\n",
        "base_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "df_train = pd.read_pickle(os.path.join(base_dir, \"embeddings_vectors_train_mri.pkl\"))\n",
        "df_val   = pd.read_pickle(os.path.join(base_dir, \"embeddings_vectors_val_mri.pkl\"))\n",
        "df_test  = pd.read_pickle(os.path.join(base_dir, \"embeddings_vectors_test_mri.pkl\"))\n",
        "\n",
        "# === Dimensiones\n",
        "dims = ['backbone', '512', '256', '128', '64']\n",
        "df_all_train = pd.concat([df_train, df_val], ignore_index=True)\n",
        "\n",
        "# === Codificar etiquetas\n",
        "le = LabelEncoder()\n",
        "df_all_train['label_enc'] = le.fit_transform(df_all_train['label'])\n",
        "df_test['label_enc'] = le.transform(df_test['label'])\n",
        "n_classes = len(le.classes_)\n",
        "\n",
        "# === Clasificadores a evaluar\n",
        "classifiers = {\n",
        "    'Logistic Regression': GridSearchCV(LogisticRegression(max_iter=1000, solver='liblinear'),\n",
        "                                        {'C': [0.01, 0.1, 1, 10, 100]}, cv=5),\n",
        "\n",
        "    'KNN': GridSearchCV(KNeighborsClassifier(),\n",
        "                        {'n_neighbors': list(range(1, 16))}, cv=5),\n",
        "\n",
        "    'Random Forest': GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                                  {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}, cv=5),\n",
        "\n",
        "    'SVM': GridSearchCV(SVC(probability=True),\n",
        "                        {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}, cv=5),\n",
        "\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "\n",
        "    'XGBoost': XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        tree_method='hist',\n",
        "        random_state=42,\n",
        "        eta=0.01,\n",
        "        max_depth=6,\n",
        "        n_estimators=100,\n",
        "    )\n",
        "}\n",
        "\n",
        "# === Resultados generales\n",
        "results = []\n",
        "\n",
        "for dim in dims:\n",
        "    print(f\"\\n === Evaluando dimensi√≥n: {dim} ===\")\n",
        "    X_train = np.stack(df_all_train[dim].values)\n",
        "    y_train = df_all_train['label_enc'].values\n",
        "    X_test = np.stack(df_test[dim].values)\n",
        "    y_test = df_test['label_enc'].values\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\n Entrenando: {name}...\")\n",
        "\n",
        "        model = clf.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        report_dict = classification_report(\n",
        "            y_test, y_pred,\n",
        "            target_names=le.classes_.astype(str),\n",
        "            digits=6,\n",
        "            output_dict=True\n",
        "        )\n",
        "        report_str = classification_report(\n",
        "            y_test, y_pred,\n",
        "            target_names=le.classes_.astype(str),\n",
        "            digits=6,\n",
        "            output_dict=False\n",
        "        )\n",
        "\n",
        "        print(f\"\\n===== Reporte {name} - Dimensi√≥n: {dim} =====\")\n",
        "        print(report_str)\n",
        "\n",
        "        results.append({\n",
        "            'dimension': dim,\n",
        "            'model': name,\n",
        "            'accuracy': acc,\n",
        "            'macro_f1': report_dict['macro avg']['f1-score'],\n",
        "            'macro_precision': report_dict['macro avg']['precision'],\n",
        "            'macro_recall': report_dict['macro avg']['recall']\n",
        "        })\n",
        "\n",
        "# === Mostrar resumen\n",
        "df_results = pd.DataFrame(results)\n",
        "pd.set_option(\"display.precision\", 6)\n",
        "print(\"\\n Tabla resumen por modelo y dimensi√≥n:\\n\")\n",
        "print(df_results)\n",
        "\n",
        "# === Guardar\n",
        "df_results.to_csv(os.path.join(base_dir, \"multi_model_metrics_summary.csv\"), index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GBP0D2bq2N4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUTmwyZwHDod"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# === Rutas\n",
        "base_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "df_train = pd.read_pickle(os.path.join(base_dir, \"embeddings_vectors_train_mri.pkl\"))\n",
        "df_val   = pd.read_pickle(os.path.join(base_dir, \"embeddings_vectors_val_mri.pkl\"))\n",
        "df_test  = pd.read_pickle(os.path.join(base_dir, \"embeddings_vectors_test_mri.pkl\"))\n",
        "\n",
        "# === Dimensiones a evaluar\n",
        "dims = ['backbone', '512', '256', '128', '64']\n",
        "\n",
        "# === Etiquetas codificadas\n",
        "le = LabelEncoder()\n",
        "df_train['y'] = le.fit_transform(df_train['label'])\n",
        "df_val['y'] = le.transform(df_val['label'])\n",
        "df_test['y'] = le.transform(df_test['label'])\n",
        "n_classes = len(le.classes_)\n",
        "\n",
        "# === M√©todos y n√∫mero de componentes\n",
        "methods = ['SDGM-D', 'DGMMC-S']\n",
        "components_list = [1, 2]\n",
        "\n",
        "# === Resultados resumen\n",
        "summary = []\n",
        "\n",
        "# === Evaluar\n",
        "for dim in dims:\n",
        "    print(f\"\\n Dimensi√≥n: {dim}\")\n",
        "\n",
        "    X_train = np.stack(df_train[dim].values)\n",
        "    y_train = df_train['y'].values\n",
        "    X_test  = np.stack(df_test[dim].values)\n",
        "    y_test  = df_test['y'].values\n",
        "\n",
        "    for method in methods:\n",
        "        for n_comp in components_list:\n",
        "            if method == 'SDGM-D':\n",
        "                gmms = []\n",
        "                for c in range(n_classes):\n",
        "                    gmm = GaussianMixture(n_components=n_comp, covariance_type='full', random_state=42)\n",
        "                    gmm.fit(X_train[y_train == c])\n",
        "                    gmms.append(gmm)\n",
        "\n",
        "                log_probs = np.array([gmm.score_samples(X_test) for gmm in gmms]).T\n",
        "                y_pred = np.argmax(log_probs, axis=1)\n",
        "\n",
        "            elif method == 'DGMMC-S':\n",
        "                gmm = GaussianMixture(n_components=n_comp * n_classes, covariance_type='full', random_state=42)\n",
        "                gmm.fit(X_train)\n",
        "\n",
        "                component_labels = np.zeros(gmm.n_components, dtype=int)\n",
        "                comp_assignments = gmm.predict(X_train)\n",
        "                for k in range(gmm.n_components):\n",
        "                    indices = np.where(comp_assignments == k)[0]\n",
        "                    if len(indices) > 0:\n",
        "                        labels_k = y_train[indices]\n",
        "                        most_common = np.bincount(labels_k).argmax()\n",
        "                        component_labels[k] = most_common\n",
        "                    else:\n",
        "                        component_labels[k] = 0\n",
        "\n",
        "                comp_preds = gmm.predict(X_test)\n",
        "                y_pred = component_labels[comp_preds]\n",
        "\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            report_dict = classification_report(\n",
        "                y_test,\n",
        "                y_pred,\n",
        "                target_names=le.classes_.astype(str),\n",
        "                digits=6,\n",
        "                output_dict=True\n",
        "            )\n",
        "            report_str = classification_report(\n",
        "                y_test,\n",
        "                y_pred,\n",
        "                target_names=le.classes_.astype(str),\n",
        "                digits=6,\n",
        "                output_dict=False\n",
        "            )\n",
        "\n",
        "            print(f\"\\n===== Reporte {method} ({n_comp} comp) - Dimensi√≥n: {dim} =====\")\n",
        "            print(report_str)\n",
        "\n",
        "            # Guardar resumen\n",
        "            summary.append({\n",
        "                'dimension': dim,\n",
        "                'method': method,\n",
        "                'components': n_comp,\n",
        "                'accuracy': acc,\n",
        "                'macro_f1': report_dict['macro avg']['f1-score'],\n",
        "                'macro_precision': report_dict['macro avg']['precision'],\n",
        "                'macro_recall': report_dict['macro avg']['recall']\n",
        "            })\n",
        "\n",
        "# === Mostrar tabla resumen\n",
        "df_summary = pd.DataFrame(summary)\n",
        "pd.set_option(\"display.precision\", 6)\n",
        "print(\"\\n Tabla resumen (accuracy, macro f1, etc):\\n\")\n",
        "print(df_summary)\n",
        "\n",
        "# === Guardar CSV (opcional)\n",
        "df_summary.to_csv(os.path.join(base_dir, \"gmm_metrics_summary.csv\"), index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF1SKmb4q2Pm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyk1lZMvrW0i"
      },
      "source": [
        "# clasificador jerakiko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClKji6XbrjUM"
      },
      "source": [
        "data augmentation en embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsfWPtE4q2TN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# === Rutas absolutas a los pickles en Google Drive\n",
        "base_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "pickle_train_path = os.path.join(base_dir, \"embeddings_vectors_train_mri.pkl\")\n",
        "pickle_val_path   = os.path.join(base_dir, \"embeddings_vectors_val_mri.pkl\")\n",
        "\n",
        "# === Cargar los embeddings\n",
        "df_train = pd.read_pickle(pickle_train_path)\n",
        "df_val   = pd.read_pickle(pickle_val_path)\n",
        "\n",
        "# === Par√°metros\n",
        "dims = ['backbone', '512', '256', '128', '64']\n",
        "N_NEIGHBORS = 5\n",
        "EPSILON = 0.1\n",
        "VAL_FRACTION = 1  # Usa 0.1 si quieres usar solo el 10% del set de validaci√≥n\n",
        "\n",
        "# === Selecci√≥n de muestras para aumento\n",
        "N = len(df_train)\n",
        "n_aug = int(len(df_val) * VAL_FRACTION)\n",
        "sel_idx = np.random.choice(N, n_aug, replace=False)\n",
        "Y_all = df_train['label'].values\n",
        "Y_sel = Y_all[sel_idx]\n",
        "\n",
        "# === Extraer vectores por dimensi√≥n\n",
        "X_all = {dim: np.stack(df_train[dim].values) for dim in dims}\n",
        "X_sel = {dim: X_all[dim][sel_idx] for dim in dims}\n",
        "\n",
        "# === Calcular vecinos (dimensi√≥n backbone)\n",
        "knn = NearestNeighbors(n_neighbors=N_NEIGHBORS + 1).fit(X_all['backbone'])\n",
        "neighbors = knn.kneighbors(X_sel['backbone'], return_distance=False)\n",
        "\n",
        "# === Funci√≥n de mezcla tipo Mixup\n",
        "def apply_mixup(x_i, x_j, alpha=0.4):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    return lam * x_i + (1 - lam) * x_j\n",
        "\n",
        "# === Construcci√≥n del nuevo dataset aumentado\n",
        "def build_augmented_df(name, strategy_fn, X_source):\n",
        "    df_aug = {'label': [], **{dim: [] for dim in dims}}\n",
        "\n",
        "    for i in range(n_aug):\n",
        "        x_aug = {}\n",
        "        y_i = Y_sel[i]\n",
        "\n",
        "        j = strategy_fn(i)\n",
        "        if j is None:\n",
        "            continue\n",
        "\n",
        "        y_j = Y_all[j]\n",
        "        for dim in dims:\n",
        "            x_i = X_sel[dim][i]\n",
        "            x_j = X_source[dim][j]\n",
        "            x_aug[dim] = apply_mixup(x_i, x_j)\n",
        "\n",
        "        label = y_i if np.random.rand() < 0.5 else y_j\n",
        "        df_aug['label'].append(label)\n",
        "        for dim in dims:\n",
        "            df_aug[dim].append(x_aug[dim])\n",
        "\n",
        "    df_out = pd.DataFrame(df_aug)\n",
        "\n",
        "    # === Guardar en Drive\n",
        "    save_path = os.path.join(base_dir, f'embedding_vectors_augmented_{name}.pkl')\n",
        "    df_out.to_pickle(save_path)\n",
        "    print(f\" Guardado: {save_path}\")\n",
        "\n",
        "# === Ejecutar Neighborhood Mixup\n",
        "build_augmented_df('neighborhood_mixup', lambda i: np.random.choice(neighbors[i][1:]), X_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BTI9SQormIJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI7DBy3tK5Xk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy import linalg\n",
        "\n",
        "# === Rutas\n",
        "BASE_DIR = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "VERSION = \"version_5\"\n",
        "CHECKPOINT_DIR = f\"/content/drive/MyDrive/Mendoza_Serey(2025)/{VERSION}\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# === Datos\n",
        "pickle_train_path = os.path.join(BASE_DIR, \"embeddings_vectors_train_mri.pkl\")\n",
        "pickle_val_path   = os.path.join(BASE_DIR, \"embeddings_vectors_val_mri.pkl\")\n",
        "pickle_test_path  = os.path.join(BASE_DIR, \"embeddings_vectors_test_mri.pkl\")\n",
        "pickle_aug_path   = os.path.join(BASE_DIR, \"embedding_vectors_augmented_neighborhood_mixup.pkl\")\n",
        "\n",
        "# === Par√°metros\n",
        "dims = ['backbone', '512', '256', '128', '64']\n",
        "N_NEIGHBORS = 10\n",
        "MAX_COMPONENTS_PER_CLASS = 10\n",
        "normalizer = Normalizer(norm='l2')\n",
        "le = LabelEncoder()\n",
        "\n",
        "# === Funci√≥n para normalizar matrices por fila\n",
        "def row_normalize(matrix):\n",
        "    row_sums = np.sum(matrix, axis=1, keepdims=True) + 1e-10\n",
        "    return matrix / row_sums\n",
        "\n",
        "# === Cargar pickles\n",
        "df_train = pd.read_pickle(pickle_train_path)\n",
        "df_val   = pd.read_pickle(pickle_val_path)\n",
        "df_test  = pd.read_pickle(pickle_test_path)\n",
        "df_aug   = pd.read_pickle(pickle_aug_path)\n",
        "df_train_aug = pd.concat([df_train, df_aug], ignore_index=True)\n",
        "\n",
        "# === Etiquetas\n",
        "y_train_raw = df_train_aug['label'].values\n",
        "y_test_raw  = df_test['label'].values\n",
        "y_val_raw   = df_val['label'].values\n",
        "y_train_enc = le.fit_transform(y_train_raw)\n",
        "y_test_enc  = le.transform(y_test_raw)\n",
        "y_val_enc   = le.transform(y_val_raw)\n",
        "joblib.dump(le, os.path.join(CHECKPOINT_DIR, 'label_encoder_version_5.pkl'))\n",
        "\n",
        "# === Procesar por dimensi√≥n\n",
        "for dim in dims:\n",
        "    print(f\"\\n Procesando dimensi√≥n: {dim}\")\n",
        "\n",
        "    # Rutas de salida\n",
        "    ftr_tr_path = os.path.join(CHECKPOINT_DIR, f'ftr_train_{dim}_version_5.npy')\n",
        "    ftr_te_path = os.path.join(CHECKPOINT_DIR, f'ftr_test_{dim}_version_5.npy')\n",
        "    ftr_val_path = os.path.join(CHECKPOINT_DIR, f'ftr_val_{dim}_version_5.npy')\n",
        "    gmm_path    = os.path.join(CHECKPOINT_DIR, f'gmm_global_{dim}_version_5.pkl')\n",
        "    map_path    = os.path.join(CHECKPOINT_DIR, f'component_to_class_{dim}_version_5.pkl')\n",
        "\n",
        "    if all(os.path.exists(p) for p in [ftr_tr_path, ftr_te_path, ftr_val_path, gmm_path, map_path]):\n",
        "        print(f\" Dimensi√≥n {dim} ya procesada. Saltando...\")\n",
        "        continue\n",
        "\n",
        "    # === Datos\n",
        "    X_train = np.stack(df_train_aug[dim].values)\n",
        "    X_test  = np.stack(df_test[dim].values)\n",
        "    X_val   = np.stack(df_val[dim].values)\n",
        "\n",
        "    X_train_norm = normalizer.fit_transform(X_train)\n",
        "    X_test_norm  = normalizer.transform(X_test)\n",
        "    X_val_norm   = normalizer.transform(X_val)\n",
        "\n",
        "    print(\" Entrenando GMMs por clase con BIC...\")\n",
        "    all_means, all_covs, all_weights, comp_to_class = [], [], [], []\n",
        "    for cls in np.unique(y_train_enc):\n",
        "        X_cls = X_train_norm[y_train_enc == cls]\n",
        "        best_gmm, best_bic = None, np.inf\n",
        "        for k in range(1, MAX_COMPONENTS_PER_CLASS + 1):\n",
        "            gmm = GaussianMixture(n_components=k, covariance_type='full', reg_covar=1e-6, random_state=42)\n",
        "            gmm.fit(X_cls)\n",
        "            bic = gmm.bic(X_cls)\n",
        "            if bic < best_bic:\n",
        "                best_bic = bic\n",
        "                best_gmm = gmm\n",
        "        preds = best_gmm.predict(X_cls)\n",
        "        for i in range(best_gmm.n_components):\n",
        "            idx = preds == i\n",
        "            if np.sum(idx) == 0:\n",
        "                continue\n",
        "            all_means.append(best_gmm.means_[i])\n",
        "            all_covs.append(best_gmm.covariances_[i])\n",
        "            all_weights.append(np.mean(idx))\n",
        "            comp_to_class.append(cls)\n",
        "\n",
        "    print(\" Consolidando GMM global y mapa componente->clase...\")\n",
        "    gmm = GaussianMixture(n_components=len(all_means), covariance_type='full', reg_covar=1e-6)\n",
        "    gmm.weights_ = np.array(all_weights) / np.sum(all_weights)\n",
        "    gmm.means_ = np.array(all_means)\n",
        "    gmm.covariances_ = np.array(all_covs)\n",
        "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(gmm.covariances_))\n",
        "\n",
        "    joblib.dump(gmm, gmm_path)\n",
        "    joblib.dump(comp_to_class, map_path)\n",
        "\n",
        "    # === Calcular pertenencias por clase\n",
        "    def get_class_probs(X):\n",
        "        probs = gmm.predict_proba(X)\n",
        "        class_probs = np.zeros((X.shape[0], len(np.unique(y_train_enc))))\n",
        "        for c, cls in enumerate(comp_to_class):\n",
        "            class_probs[:, cls] += probs[:, c]\n",
        "        return row_normalize(class_probs)\n",
        "\n",
        "    print(\" Buscando vecinos en set de entrenamiento...\")\n",
        "    knn = NearestNeighbors(n_neighbors=N_NEIGHBORS)\n",
        "    knn.fit(X_train_norm)\n",
        "\n",
        "    def enrich(X_query, X_base):\n",
        "        _, idxs = knn.kneighbors(X_query)\n",
        "        feats = []\n",
        "        for i, x in enumerate(X_query):\n",
        "            row = []\n",
        "            row.extend(x)\n",
        "            row.extend(get_class_probs(x.reshape(1, -1))[0])\n",
        "            for idx in idxs[i]:\n",
        "                row.extend(X_base[idx])\n",
        "                row.extend(get_class_probs(X_base[idx].reshape(1, -1))[0])\n",
        "            feats.append(row)\n",
        "        return np.array(feats)\n",
        "\n",
        "    print(\" Enriqueciendo caracter√≠sticas...\")\n",
        "    ftr_tr  = enrich(X_train_norm, X_train_norm)\n",
        "    ftr_te  = enrich(X_test_norm,  X_train_norm)\n",
        "    ftr_val = enrich(X_val_norm,   X_train_norm)\n",
        "\n",
        "    np.save(ftr_tr_path,  ftr_tr)\n",
        "    np.save(ftr_te_path,  ftr_te)\n",
        "    np.save(ftr_val_path, ftr_val)\n",
        "    print(f\" Guardado: {ftr_tr_path}, {ftr_te_path}, {ftr_val_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS2EjhXisYXA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baei92yJk4vz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Mendoza_Serey(2025)/version_5\"\n",
        "\n",
        "# Cargar pickles\n",
        "df_train = pd.read_pickle(os.path.join(BASE_DIR, \"embeddings_vectors_train_mri.pkl\"))\n",
        "df_aug   = pd.read_pickle(os.path.join(BASE_DIR, \"embedding_vectors_augmented_neighborhood_mixup.pkl\"))\n",
        "df_val   = pd.read_pickle(os.path.join(BASE_DIR, \"embeddings_vectors_val_mri.pkl\"))\n",
        "df_test  = pd.read_pickle(os.path.join(BASE_DIR, \"embeddings_vectors_test_mri.pkl\"))\n",
        "\n",
        "\n",
        "    ftr_tr_path = os.path.join(CHECKPOINT_DIR, f'ftr_train_{dim}_version_5.npy')\n",
        "    ftr_te_path = os.path.join(CHECKPOINT_DIR, f'ftr_test_{dim}_version_5.npy')\n",
        "    ftr_val_path = os.path.join(CHECKPOINT_DIR, f'ftr_val_{dim}_version_5.npy')\n",
        "    gmm_path    = os.path.join(CHECKPOINT_DIR, f'gmm_global_{dim}_version_5.pkl')\n",
        "    map_path    = os.path.join(CHECKPOINT_DIR, f'component_to_class_{dim}_version_5.pkl')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_train_aug = pd.concat([df_train, df_aug], ignore_index=True)\n",
        "\n",
        "# Codificar etiquetas\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(df_train_aug['label'].values)\n",
        "y_val_enc   = le.transform(df_val['label'].values)\n",
        "y_test_enc  = le.transform(df_test['label'].values)\n",
        "\n",
        "# Guardar\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_train.npy'), y_train_enc)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_val.npy'),   y_val_enc)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_test.npy'),  y_test_enc)\n",
        "joblib.dump(le, os.path.join(CHECKPOINT_DIR, 'label_encoder_version_5.pkl'))\n",
        "\n",
        "print(\" Etiquetas recuperadas y guardadas exitosamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RljcMXjNskQ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Ruta\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/Mendoza_Serey(2025)/version_5'\n",
        "dims = ['backbone', '512', '256', '128', '64']\n",
        "\n",
        "# === Cargar etiquetas\n",
        "df_train = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embeddings_vectors_train_mri.pkl'))\n",
        "df_test  = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embeddings_vectors_test_mri.pkl'))\n",
        "df_val   = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embeddings_vectors_val_mri.pkl'))\n",
        "df_aug   = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embedding_vectors_augmented_neighborhood_mixup.pkl'))\n",
        "\n",
        "# Concatenar entrenamiento + augmentaci√≥n\n",
        "df_train_aug = pd.concat([df_train, df_aug], ignore_index=True)\n",
        "\n",
        "# === Codificar etiquetas\n",
        "le = joblib.load(os.path.join(CHECKPOINT_DIR, 'label_encoder_version_5.pkl'))\n",
        "y_train = le.transform(df_train_aug['label'].values)\n",
        "y_test  = le.transform(df_test['label'].values)\n",
        "\n",
        "# === Concatenar caracter√≠sticas enriquecidas por dimensi√≥n\n",
        "def concat_features(split):\n",
        "    feats = []\n",
        "    for dim in dims:\n",
        "        fpath = os.path.join(CHECKPOINT_DIR, f'ftr_{split}_{dim}_version_5.npy')\n",
        "        feats.append(np.load(fpath))\n",
        "    return np.hstack(feats)\n",
        "\n",
        "print(\"üîÑ Concatenando caracter√≠sticas enriquecidas...\")\n",
        "X_train = concat_features(\"train\")\n",
        "X_test  = concat_features(\"test\")\n",
        "\n",
        "# === Guardar para entrenamiento\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'X_train_version_5.npy'), X_train)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'X_test_version_5.npy'),  X_test)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_train_version_5.npy'), y_train)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_test_version_5.npy'),  y_test)\n",
        "\n",
        "print(\"‚úÖ Datos jer√°rquicos concatenados y guardados.\")\n",
        "print(f\"   X_train shape: {X_train.shape}\")\n",
        "print(f\"   X_test shape : {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT6f1l_Pua8p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# === Ruta en tu Google Drive\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/Mendoza_Serey(2025)/version_5'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# === Cargar datos y codificador\n",
        "X_train = np.load(os.path.join(CHECKPOINT_DIR, 'X_train_version_5.npy'))\n",
        "X_test  = np.load(os.path.join(CHECKPOINT_DIR, 'X_test_version_5.npy'))\n",
        "y_train = np.load(os.path.join(CHECKPOINT_DIR, 'y_train_version_5.npy'))\n",
        "y_test  = np.load(os.path.join(CHECKPOINT_DIR, 'y_test_version_5.npy'))\n",
        "le      = joblib.load(os.path.join(CHECKPOINT_DIR, 'label_encoder_version_5.pkl'))\n",
        "\n",
        "# === Crear DMatrix para XGBoost\n",
        "dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# === Hiperpar√°metros √≥ptimos (de Optuna Trial 0)\n",
        "best_params = {\n",
        "    \"objective\": \"multi:softprob\",\n",
        "    \"num_class\": len(le.classes_),\n",
        "    \"tree_method\": \"hist\",  # usa \"gpu_hist\" si tienes CUDA\n",
        "    \"device\": \"cuda\",       # ignorable si no hay GPU\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "    \"seed\": 42,\n",
        "    \"eta\": 0.0013292918943162175,\n",
        "    \"max_depth\": 8,\n",
        "    \"min_child_weight\": 8,\n",
        "    \"subsample\": 0.7993292420985183,\n",
        "    \"colsample_bytree\": 0.5780093202212182,\n",
        "    \"gamma\": 1.7699302940633311e-07,\n",
        "    \"alpha\": 2.9152036385288193e-08,\n",
        "    \"lambda\": 0.08499808989182997,\n",
        "    \"max_bin\": 64\n",
        "}\n",
        "\n",
        "# === Entrenar el modelo completo\n",
        "bst_final = xgb.train(\n",
        "    best_params,\n",
        "    dtrain_full,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain_full, \"train\")],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose_eval=True\n",
        ")\n",
        "\n",
        "# === Guardar el modelo entrenado\n",
        "model_path = os.path.join(CHECKPOINT_DIR, 'xgboost_booster_version_5_optuna_retrain.json')\n",
        "bst_final.save_model(model_path)\n",
        "print(f\" Modelo guardado en: {model_path}\")\n",
        "\n",
        "# === Evaluaci√≥n final sobre el conjunto de prueba\n",
        "preds_prob = bst_final.predict(dtest)\n",
        "preds = np.argmax(preds_prob, axis=1)\n",
        "\n",
        "# === Reporte de clasificaci√≥n\n",
        "report = classification_report(\n",
        "    y_test, preds,\n",
        "    target_names=le.classes_.astype(str),\n",
        "    output_dict=True\n",
        ")\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "# === Guardar reporte\n",
        "report_path = os.path.join(CHECKPOINT_DIR, 'report_version_5_optuna_retrain.csv')\n",
        "df_report.to_csv(report_path)\n",
        "\n",
        "print(f\" Reporte guardado en: {report_path}\")\n",
        "print(df_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#try 2"
      ],
      "metadata": {
        "id": "0JXgwwGhdO3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Ruta\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/Mendoza_Serey(2025)/version_5'\n",
        "DIM_BACKBONE = 'backbone'\n",
        "\n",
        "# === Cargar etiquetas\n",
        "df_train = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embeddings_vectors_train_mri.pkl'))\n",
        "df_test  = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embeddings_vectors_test_mri.pkl'))\n",
        "df_val   = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embeddings_vectors_val_mri.pkl'))\n",
        "df_aug   = pd.read_pickle(os.path.join(CHECKPOINT_DIR, '../embeddings/embedding_vectors_augmented_neighborhood_mixup.pkl'))\n",
        "\n",
        "# Concatenar entrenamiento + augmentaci√≥n\n",
        "df_train_aug = pd.concat([df_train, df_aug], ignore_index=True)\n",
        "\n",
        "# === Codificar etiquetas\n",
        "le = joblib.load(os.path.join(CHECKPOINT_DIR, 'label_encoder_version_5.pkl'))\n",
        "y_train = le.transform(df_train_aug['label'].values)\n",
        "y_test  = le.transform(df_test['label'].values)\n",
        "\n",
        "# === Cargar solo la dimensi√≥n backbone\n",
        "def load_backbone_features(split):\n",
        "    fpath = os.path.join(CHECKPOINT_DIR, f'ftr_{split}_{DIM_BACKBONE}_version_5.npy')\n",
        "    return np.load(fpath)\n",
        "\n",
        "print(\"üîÑ Cargando caracter√≠sticas solo de dimensi√≥n 'backbone'...\")\n",
        "X_train_backbone = load_backbone_features(\"train\")\n",
        "X_test_backbone  = load_backbone_features(\"test\")\n",
        "\n",
        "# === Guardar nuevos archivos con solo backbone\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'X_train_backbone_version_5.npy'), X_train_backbone)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'X_test_backbone_version_5.npy'),  X_test_backbone)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_train_backbone_version_5.npy'), y_train)\n",
        "np.save(os.path.join(CHECKPOINT_DIR, 'y_test_backbone_version_5.npy'),  y_test)\n",
        "\n",
        "print(\"‚úÖ Caracter√≠sticas 'backbone' guardadas.\")\n",
        "print(f\"   X_train_backbone shape: {X_train_backbone.shape}\")\n",
        "print(f\"   X_test_backbone shape : {X_test_backbone.shape}\")\n"
      ],
      "metadata": {
        "id": "HAhH7byfdCgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# === Ruta en tu Google Drive\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/Mendoza_Serey(2025)/version_5'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# === Cargar datos backbone y codificador\n",
        "X_train = np.load(os.path.join(CHECKPOINT_DIR, 'X_train_backbone_version_5.npy'))\n",
        "X_test  = np.load(os.path.join(CHECKPOINT_DIR, 'X_test_backbone_version_5.npy'))\n",
        "y_train = np.load(os.path.join(CHECKPOINT_DIR, 'y_train_backbone_version_5.npy'))\n",
        "y_test  = np.load(os.path.join(CHECKPOINT_DIR, 'y_test_backbone_version_5.npy'))\n",
        "le      = joblib.load(os.path.join(CHECKPOINT_DIR, 'label_encoder_version_5.pkl'))\n",
        "\n",
        "# === Crear DMatrix para XGBoost\n",
        "dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# === Hiperpar√°metros √≥ptimos (de Optuna Trial 0)\n",
        "best_params = {\n",
        "    \"objective\": \"multi:softprob\",\n",
        "    \"num_class\": len(le.classes_),\n",
        "    \"tree_method\": \"hist\",  # usa \"gpu_hist\" si tienes CUDA\n",
        "    \"device\": \"cuda\",       # ignorable si no hay GPU\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "    \"seed\": 42,\n",
        "    \"eta\": 0.0013292918943162175,\n",
        "    \"max_depth\": 8,\n",
        "    \"min_child_weight\": 8,\n",
        "    \"subsample\": 0.7993292420985183,\n",
        "    \"colsample_bytree\": 0.5780093202212182,\n",
        "    \"gamma\": 1.7699302940633311e-07,\n",
        "    \"alpha\": 2.9152036385288193e-08,\n",
        "    \"lambda\": 0.08499808989182997,\n",
        "    \"max_bin\": 64\n",
        "}\n",
        "\n",
        "# === Entrenar el modelo completo\n",
        "bst_final = xgb.train(\n",
        "    best_params,\n",
        "    dtrain_full,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain_full, \"train\")],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose_eval=True\n",
        ")\n",
        "\n",
        "# === Guardar el modelo entrenado (versi√≥n backbone)\n",
        "model_path = os.path.join(CHECKPOINT_DIR, 'xgboost_booster_backbone_version_5.json')\n",
        "bst_final.save_model(model_path)\n",
        "print(f\"‚úÖ Modelo backbone guardado en: {model_path}\")\n",
        "\n",
        "# === Evaluaci√≥n final sobre el conjunto de prueba\n",
        "preds_prob = bst_final.predict(dtest)\n",
        "preds = np.argmax(preds_prob, axis=1)\n",
        "\n",
        "# === Reporte de clasificaci√≥n\n",
        "report = classification_report(\n",
        "    y_test, preds,\n",
        "    target_names=le.classes_.astype(str),\n",
        "    output_dict=True\n",
        ")\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "# === Guardar reporte (versi√≥n backbone)\n",
        "report_path = os.path.join(CHECKPOINT_DIR, 'report_backbone_version_5.csv')\n",
        "df_report.to_csv(report_path)\n",
        "\n",
        "print(f\"‚úÖ Reporte backbone guardado en: {report_path}\")\n",
        "print(df_report)\n"
      ],
      "metadata": {
        "id": "feHEfX60dQ9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# === Ruta del archivo con embeddings del set de testing\n",
        "pkl_path = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings/embeddings_vectors_train_mri.pkl\"\n",
        "\n",
        "# === Cargar embeddings\n",
        "df_test = pd.read_pickle(pkl_path)\n",
        "X = df_test['64'].tolist()\n",
        "y = df_test['label'].tolist()\n",
        "\n",
        "# === Convertir a NumPy\n",
        "X = np.vstack(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# === Aplicar t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca')\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# === Visualizar\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='tab10', s=60, alpha=0.8)\n",
        "plt.title(\"t-SNE de Embeddings (dim=64) - Set de Test\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE 1\")\n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "plt.legend(title=\"Clase\", loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0y7eKF-cvT0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# === Ruta del archivo con embeddings del set de testing\n",
        "pkl_path = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings/embeddings_vectors_test_mri.pkl\"\n",
        "\n",
        "# === Cargar DataFrame de embeddings\n",
        "df_test = pd.read_pickle(pkl_path)\n",
        "\n",
        "# === Lista de dimensiones\n",
        "dimensiones = ['backbone', '512', '256', '128', '64']\n",
        "label = df_test['label'].tolist()\n",
        "\n",
        "# === Visualizar cada dimensi√≥n con PCA\n",
        "for dim in dimensiones:\n",
        "    X = np.vstack(df_test[dim].values)\n",
        "    y = np.array(label)\n",
        "\n",
        "    # Aplicar PCA\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='tab10', s=60, alpha=0.8)\n",
        "    plt.title(f\"PCA - Dimensi√≥n {dim}\", fontsize=14)\n",
        "    plt.xlabel(\"PCA 1\")\n",
        "    plt.ylabel(\"PCA 2\")\n",
        "    plt.legend(title=\"Clase\", loc=\"best\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BTnRbEjtvzHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# === Ruta donde tienes los embeddings ===\n",
        "save_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "file_path = os.path.join(save_dir, \"embeddings_vectors_train_mri.pkl\")\n",
        "\n",
        "# === Cargar DataFrame de embeddings ===\n",
        "df = pd.read_pickle(file_path)\n",
        "\n",
        "# === Columnas que contienen los embeddings ===\n",
        "embedding_keys = [\"backbone\", \"512\", \"256\", \"128\", \"64\"]\n",
        "\n",
        "# === Crear figura con subplots ===\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, key in enumerate(embedding_keys):\n",
        "    X = np.vstack(df[key].values)   # Convierte lista de vectores en matriz (n_samples, n_features)\n",
        "    y = df[\"label\"].values\n",
        "\n",
        "    # PCA a 2 componentes\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Scatter plot\n",
        "    sc = axes[i].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"tab10\", s=10, alpha=0.7)\n",
        "    axes[i].set_title(f\"PCA - {key} ({X.shape[1]}D)\", fontsize=12)\n",
        "    axes[i].set_xlabel(\"PC1\")\n",
        "    axes[i].set_ylabel(\"PC2\")\n",
        "\n",
        "# Leyenda\n",
        "handles, labels = sc.legend_elements()\n",
        "fig.legend(handles, labels, title=\"Clases\", bbox_to_anchor=(1.02, 0.9))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zqMf-q6Vk6zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# === Ruta ===\n",
        "save_dir = \"/content/drive/MyDrive/Mendoza_Serey(2025)/embeddings\"\n",
        "file_path = os.path.join(save_dir, \"embeddings_vectors_train_mri.pkl\")\n",
        "\n",
        "# === Cargar DataFrame de embeddings ===\n",
        "df = pd.read_pickle(file_path)\n",
        "\n",
        "# === Columnas de embeddings ===\n",
        "embedding_keys = [\"backbone\", \"512\", \"256\", \"128\", \"64\"]\n",
        "\n",
        "for key in embedding_keys:\n",
        "    X = np.vstack(df[key].values)   # Matriz (n_samples, n_features)\n",
        "    y = df[\"label\"].values\n",
        "\n",
        "    # PCA 2D\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Figura individual\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sc = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"tab10\", s=10, alpha=0.7)\n",
        "\n",
        "    plt.title(f\"PCA - {key} ({X.shape[1]}D)\", fontsize=14)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "\n",
        "    # Leyenda individual (a la derecha)\n",
        "    handles, labels = sc.legend_elements()\n",
        "    plt.legend(handles, labels, title=\"Clases\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2G7w6n5jmU5D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}